<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Writing Data &#8212; OpenTSDB 2.4 documentation</title>
    <link rel="stylesheet" href="../../_static/solar.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Querying or Reading Data" href="../query/index.html" />
    <link rel="prev" title="Configuration" href="../configuration.html" /><link href='http://fonts.googleapis.com/css?family=Source+Code+Pro|Open+Sans:300italic,400italic,700italic,400,300,700' rel='stylesheet' type='text/css'>
<link href="../../_static/solarized-dark.css" rel="stylesheet">
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-18339382-1']);
  _gaq.push(['_setDomainName', 'none']);
  _gaq.push(['_setAllowLinker', true]);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../query/index.html" title="Querying or Reading Data"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../configuration.html" title="Configuration"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTSDB 2.4 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" accesskey="U">User Guide</a> &#187;</li> 
      </ul>
    </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="../../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Writing Data</a><ul>
<li><a class="reference internal" href="#naming-schema">Naming Schema</a><ul>
<li><a class="reference internal" href="#aggregations">Aggregations</a></li>
<li><a class="reference internal" href="#time-series-cardinality">Time Series Cardinality</a></li>
<li><a class="reference internal" href="#naming-conclusion">Naming Conclusion</a></li>
</ul>
</li>
<li><a class="reference internal" href="#data-specification">Data Specification</a><ul>
<li><a class="reference internal" href="#timestamps">Timestamps</a></li>
<li><a class="reference internal" href="#metrics-and-tags">Metrics and Tags</a></li>
<li><a class="reference internal" href="#integer-values">Integer Values</a></li>
<li><a class="reference internal" href="#floating-point-values">Floating Point Values</a></li>
<li><a class="reference internal" href="#ordering">Ordering</a></li>
<li><a class="reference internal" href="#duplicate-data-points">Duplicate Data Points</a></li>
</ul>
</li>
<li><a class="reference internal" href="#input-methods">Input Methods</a><ul>
<li><a class="reference internal" href="#telnet">Telnet</a></li>
<li><a class="reference internal" href="#http-api">Http API</a></li>
<li><a class="reference internal" href="#batch-import">Batch Import</a></li>
</ul>
</li>
<li><a class="reference internal" href="#write-performance">Write Performance</a><ul>
<li><a class="reference internal" href="#uid-assignment">UID Assignment</a></li>
<li><a class="reference internal" href="#random-metric-uid-assignment">Random Metric UID Assignment</a></li>
<li><a class="reference internal" href="#salting">Salting</a></li>
<li><a class="reference internal" href="#appends">Appends</a></li>
<li><a class="reference internal" href="#pre-split-hbase-regions">Pre-Split HBase Regions</a></li>
<li><a class="reference internal" href="#distributed-hbase">Distributed HBase</a></li>
<li><a class="reference internal" href="#multiple-tsds">Multiple TSDs</a></li>
<li><a class="reference internal" href="#persistent-connections">Persistent Connections</a></li>
<li><a class="reference internal" href="#disable-meta-data-and-real-time-publishing">Disable Meta Data and Real Time Publishing</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="../configuration.html"
                        title="previous chapter">Configuration</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="../query/index.html"
                        title="next chapter">Querying or Reading Data</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="writing-data">
<h1>Writing Data</h1>
<p>You may want to jump right in and start throwing data into your TSD, but to really take advantage of OpenTSDB’s power and flexibility, you may want to pause and think about your naming schema. After you’ve done that, you can proceed to pushing data over the Telnet or HTTP APIs, or use an existing tool with OpenTSDB support such as ‘tcollector’.</p>
<div class="section" id="naming-schema">
<h2>Naming Schema</h2>
<p>Many metrics administrators are used to supplying a single name for their time series. For example, systems administrators used to RRD-style systems may name their time series <code class="docutils literal notranslate"><span class="pre">webserver01.sys.cpu.0.user</span></code>. The name tells us that the time series is recording the amount of time in user space for cpu <code class="docutils literal notranslate"><span class="pre">0</span></code> on <code class="docutils literal notranslate"><span class="pre">webserver01</span></code>. This works great if you want to retrieve just the user time for that cpu core on that particular web server later on.</p>
<p>But what if the web server has 64 cores and you want to get the average time across all of them? Some systems allow you to specify a wild card such as <code class="docutils literal notranslate"><span class="pre">webserver01.sys.cpu.*.user</span></code> that would read all 64 files and aggregate the results. Alternatively, you could record a new time series called <code class="docutils literal notranslate"><span class="pre">webserver01.sys.cpu.user.all</span></code> that represents the same aggregate but you must now write ‘64 + 1’ different time series. What if you had a thousand web servers and you wanted the average cpu time for all of your servers? You could craft a wild card query like <code class="docutils literal notranslate"><span class="pre">*.sys.cpu.*.user</span></code> and the system would open all 64,000 files, aggregate the results and return the data. Or you setup a process to pre-aggregate the data and write it to <code class="docutils literal notranslate"><span class="pre">webservers.sys.cpu.user.all</span></code>.</p>
<p>OpenTSDB handles things a bit differently by introducing the idea of ‘tags’. Each time series still has a ‘metric’ name, but it’s much more generic, something that can be shared by many unique time series. Instead, the uniqueness comes from a combination of tag key/value pairs that allows for flexible queries with very fast aggregations.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Every time series in OpenTSDB must have at least one tag.</p>
</div>
<p>Take the previous example where the metric was <code class="docutils literal notranslate"><span class="pre">webserver01.sys.cpu.0.user</span></code>. In OpenTSDB, this may become <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,</span> <span class="pre">cpu=0</span></code>. Now if we want the data for an individual core, we can craft a query like <code class="docutils literal notranslate"><span class="pre">sum:sys.cpu.user{host=webserver01,cpu=42}</span></code>. If we want all of the cores, we simply drop the cpu tag and ask for <code class="docutils literal notranslate"><span class="pre">sum:sys.cpu.user{host=webserver01}</span></code>. This will give us the aggregated results for all 64 cores. If we want the results for all 1,000 servers, we simply request <code class="docutils literal notranslate"><span class="pre">sum:sys.cpu.user</span></code>. The underlying data schema will store all of the <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span></code> time series next to each other so that aggregating the individual values is very fast and efficient. OpenTSDB was designed to make these aggregate queries as fast as possible since most users start out at a high level, then drill down for detailed information.</p>
<div class="section" id="aggregations">
<h3>Aggregations</h3>
<p>While the tagging system is flexible, some problems can arise if you don’t understand the querying side of OpenTSDB, hence the need for some forethought. Take the example query above: <code class="docutils literal notranslate"><span class="pre">sum:sys.cpu.user{host=webserver01}</span></code>. We recorded 64 unique time series for <code class="docutils literal notranslate"><span class="pre">webserver01</span></code>, one time series for each of the CPU cores. When we issued that query, all of the time series for metric <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span></code> with the tag <code class="docutils literal notranslate"><span class="pre">host=webserver01</span></code> were retrieved, averaged, and returned as one series of numbers. Let’s say the resulting average was <code class="docutils literal notranslate"><span class="pre">50</span></code> for timestamp <code class="docutils literal notranslate"><span class="pre">1356998400</span></code>. Now we were migrating from another system to OpenTSDB and had a process that pre-aggregated all 64 cores so that we could quickly get the average value and simply wrote a new time series <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01</span></code>. If we run the same query, we’ll get a value of <code class="docutils literal notranslate"><span class="pre">100</span></code> at <code class="docutils literal notranslate"><span class="pre">1356998400</span></code>. What happened? OpenTSDB aggregated all 64 time series <em>and</em> the pre-aggregated time series to get to that 100. In storage, we would have something like this:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">sys</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">user</span> <span class="n">host</span><span class="o">=</span><span class="n">webserver01</span>        <span class="mi">1356998400</span>  <span class="mi">50</span>
<span class="n">sys</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">user</span> <span class="n">host</span><span class="o">=</span><span class="n">webserver01</span><span class="p">,</span><span class="n">cpu</span><span class="o">=</span><span class="mi">0</span>  <span class="mi">1356998400</span>  <span class="mi">1</span>
<span class="n">sys</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">user</span> <span class="n">host</span><span class="o">=</span><span class="n">webserver01</span><span class="p">,</span><span class="n">cpu</span><span class="o">=</span><span class="mi">1</span>  <span class="mi">1356998400</span>  <span class="mi">0</span>
<span class="n">sys</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">user</span> <span class="n">host</span><span class="o">=</span><span class="n">webserver01</span><span class="p">,</span><span class="n">cpu</span><span class="o">=</span><span class="mi">2</span>  <span class="mi">1356998400</span>  <span class="mi">2</span>
<span class="n">sys</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">user</span> <span class="n">host</span><span class="o">=</span><span class="n">webserver01</span><span class="p">,</span><span class="n">cpu</span><span class="o">=</span><span class="mi">3</span>  <span class="mi">1356998400</span>  <span class="mi">0</span>
<span class="o">...</span>
<span class="n">sys</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">user</span> <span class="n">host</span><span class="o">=</span><span class="n">webserver01</span><span class="p">,</span><span class="n">cpu</span><span class="o">=</span><span class="mi">63</span> <span class="mi">1356998400</span>  <span class="mi">1</span>
</pre></div>
</div>
<p>OpenTSDB will <em>automatically</em> aggregate <em>all</em> of the time series for the metric in a query if no tags are given. If one or more tags are defined, the aggregate will ‘include all’ time series that match on that tag, regardless of other tags. With the query <code class="docutils literal notranslate"><span class="pre">sum:sys.cpu.user{host=webserver01}</span></code>, we would include <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,cpu=0</span></code> as well as <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,cpu=0,manufacturer=Intel</span></code>, <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,foo=bar</span></code> and <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,cpu=0,datacenter=lax,department=ops</span></code>. The moral of this example is: <em>be careful with your naming schema</em>.</p>
</div>
<div class="section" id="time-series-cardinality">
<h3>Time Series Cardinality</h3>
<p>A critical aspect of any naming schema is to consider the cardinality of your time series. Cardinality is defined as the number of unique items in a set. In OpenTSDB’s case, this means the number of items associated with a metric, i.e. all of the possible tag name and value combinations, as well as the number of unique metric names, tag names and tag values. Cardinality is important for two reasons outlined below.</p>
<p><strong>Limited Unique IDs (UIDs)</strong></p>
<p>There is a limited number of unique IDs to assign for each metric, tag name and tag value. By default there are just over 16 million possible IDs per type. If, for example, you ran a very popular web service and tried to track the IP address of clients as a tag, e.g. <code class="docutils literal notranslate"><span class="pre">web.app.hits</span> <span class="pre">clientip=38.26.34.10</span></code>, you may quickly run into the UID assignment limit as there are over 4 billion possible IP version 4 addresses. Additionally, this approach would lead to creating a very sparse time series as the user at address <code class="docutils literal notranslate"><span class="pre">38.26.34.10</span></code> may only use your app sporadically, or perhaps never again from that specific address.</p>
<p>For small installations with tags that rarely change (e.g. stock symbols or a fixed set of sensors), the UID size may not be an issue. A tag value is assigned a UID that is completely disassociated from its tag name. If you use numeric identifiers for tag values, the number is assigned a UID once and can be used with many tag names. For example, if we assign a UID to the number <code class="docutils literal notranslate"><span class="pre">2</span></code>, we could store timeseries with the tag pairs <code class="docutils literal notranslate"><span class="pre">cpu=2</span></code>, <code class="docutils literal notranslate"><span class="pre">interface=2</span></code>, <code class="docutils literal notranslate"><span class="pre">hdd=2</span></code> and <code class="docutils literal notranslate"><span class="pre">fan=2</span></code> while consuming only 1 tag value UID (<code class="docutils literal notranslate"><span class="pre">2</span></code>) and 4 tag name UIDs (<code class="docutils literal notranslate"><span class="pre">cpu</span></code>, <code class="docutils literal notranslate"><span class="pre">interface</span></code>, <code class="docutils literal notranslate"><span class="pre">hdd</span></code> and <code class="docutils literal notranslate"><span class="pre">fan</span></code>).</p>
<p>If you think that the UID limit may impact you, first think about the queries that you want to execute. If we look at the <code class="docutils literal notranslate"><span class="pre">web.app.hits</span></code> example above, you probably only care about the total number of hits to your service and rarely need to drill down to a specific IP address. In that case, you may want to store the IP address as an annotation. That way you could still benefit from low cardinality but if you need to, you could search the results for that particular IP using external scripts.</p>
<p>When storing data for sources that do have high or changing cardinality (e.g. a Docker swarm) then you can change the UID widths by setting <code class="docutils literal notranslate"><span class="pre">tsd.storage.uid.width.metric</span></code>, <code class="docutils literal notranslate"><span class="pre">tsd.storage.uid.width.tagk</span></code> or <code class="docutils literal notranslate"><span class="pre">tsd.storage.uid.width.tagv</span></code> (maximum value is 7). You can ONLY do this when creating a new TSDB installation.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>It is possible that your situation requires this value to be increased.  If you choose to modify this value, you must start with fresh data and a new UID table. Any data written with a TSD expecting 3-byte UID encoding will be incompatible with this change, so ensure that all of your TSDs are running with the same configuration and that any data you have stored in OpenTSDB prior to making this change has been exported to a location where it can be manipulated by external tools.</p>
</div>
<p><strong>Query Speed</strong></p>
<p>Cardinality also affects query speed a great deal, so consider the queries you will be performing frequently and optimize your naming schema for those. OpenTSDB creates a new row per time series per hour. If we have one host with a single core that emits one time series <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span> <span class="pre">host=webserver01,cpu=0</span></code> with data written every second for 1 day, that would result in 24 rows of data or 86,400 data points. However if we have 8 possible CPU cores for that host, now we have 192 rows and 691,200 data points. This looks good because we can get easily a sum or average of CPU usage across all cores by issuing a query like <code class="docutils literal notranslate"><span class="pre">start=1d-ago&amp;m=avg:sys.cpu.user{host=webserver01}</span></code>. The query will iterate over all 192 rows and aggregate the data into a single time series.</p>
<p>However what if we have 20,000 hosts, each with 8 cores? Now we will have 3.8 million rows and 1.728 billion data points per day due to a high cardinality of host values. Queries for the average core usage on host <code class="docutils literal notranslate"><span class="pre">webserver01</span></code> will be slower as it must pick out 192 rows out of 3.8 million. (However with OpenTSDB 2.2, you can use the explicit tags feature to specify <code class="docutils literal notranslate"><span class="pre">cpu=*</span></code> and the fuzzy filter will kick in to help skip those unnecessary rows quicker.)</p>
<p>The benefits of this schema are that you have very deep granularity in your data, e.g., storing usage metrics on a per-core basis. You can also easily craft a query to get the average usage across all cores an all hosts: <code class="docutils literal notranslate"><span class="pre">start=1d-ago&amp;m=avg:sys.cpu.user</span></code>. However queries against that particular metric will take longer as there are more rows to sift through. This is common amongst all databases and is not OpenTSDB’s problem alone.</p>
<p>Here are some common means of dealing with cardinality:</p>
<p><strong>Pre-Aggregate</strong> - In the example above with <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span></code>, you generally care about the average usage on the host, not the usage per core. While the data collector may send a separate value per core with the tagging schema above, the collector could also send one extra data point such as <code class="docutils literal notranslate"><span class="pre">sys.cpu.user.avg</span> <span class="pre">host=webserver01</span></code>. Now you have a completely separate timeseries that would only have 24 rows per day and with 20K hosts, only 480K rows to sift through. Queries will be much more responsive for the per-host average and you still have per-core data to drill down to separately.</p>
<p><strong>Shift to Metric</strong> - What if you really only care about the metrics for a particular host and don’t need to aggregate across hosts? In that case you can shift the hostname to the metric. Our previous example becomes <code class="docutils literal notranslate"><span class="pre">sys.cpu.user.websvr01</span> <span class="pre">cpu=0</span></code>. Queries against this schema are very fast as there would only be 192 rows per day for the metric. However to aggregate across hosts you would have to execute multiple queries and aggregate outside of OpenTSDB. (Future work will include this capability).</p>
</div>
<div class="section" id="naming-conclusion">
<h3>Naming Conclusion</h3>
<p>When you design your naming schema, keep these suggestions in mind:</p>
<ul class="simple">
<li><p>Be consistent with your naming to reduce duplication. Always use the same case for metrics, tag names and values.</p></li>
<li><p>Use the same number and type of tags for each metric. E.g. don’t store <code class="docutils literal notranslate"><span class="pre">my.metric</span> <span class="pre">host=foo</span></code> and <code class="docutils literal notranslate"><span class="pre">my.metric</span> <span class="pre">datacenter=lga</span></code>.</p></li>
<li><p>Think about the most common queries you’ll be executing and optimize your schema for those queries</p></li>
<li><p>Think about how you may want to drill down when querying</p></li>
<li><p>Don’t use too many tags, keep it to a fairly small number, usually up to 4 or 5 tags (By default, OpenTSDB supports a maximum of 8 tags). If absolutely needed, you _can_ increase the number of tags available for your cluster at any time.</p></li>
</ul>
</div>
</div>
<div class="section" id="data-specification">
<h2>Data Specification</h2>
<p>Every time series data point requires the following data:</p>
<ul class="simple">
<li><p>metric - A generic name for the time series such as <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span></code>, <code class="docutils literal notranslate"><span class="pre">stock.quote</span></code> or <code class="docutils literal notranslate"><span class="pre">env.probe.temp</span></code>.</p></li>
<li><p>timestamp - A Unix/POSIX Epoch timestamp in seconds or milliseconds defined as the number of seconds that have elapsed since January 1st, 1970 at 00:00:00 UTC time. Only positive timestamps are supported at this time.</p></li>
<li><p>value - A numeric value to store at the given timestamp for the time series. This may be an integer or a floating point value.</p></li>
<li><p>tag(s) - A key/value pair consisting of a <code class="docutils literal notranslate"><span class="pre">tagk</span></code> (the key) and a <code class="docutils literal notranslate"><span class="pre">tagv</span></code> (the value). Each data point must have at least one tag.</p></li>
</ul>
<div class="section" id="timestamps">
<h3>Timestamps</h3>
<p>Data can be written to OpenTSDB with second or millisecond resolution. Timestamps must be integers and be no longer than 13 digits (See first [NOTE] below).  Millisecond timestamps must be of the format <code class="docutils literal notranslate"><span class="pre">1364410924250</span></code> where the final three digits represent the milliseconds.  Applications that generate timestamps with more than 13 digits (i.e., greater than millisecond resolution) must be rounded to a maximum of 13 digits before submitting or an error will be generated.</p>
<p>Timestamps with second resolution are stored on 2 bytes while millisecond resolution are stored on 4. Thus if you do not need millisecond resolution or all of your data points are on 1 second boundaries, we recommend that you submit timestamps with 10 digits for second resolution so that you can save on storage space. It’s also a good idea to avoid mixing second and millisecond timestamps for a given time series. Doing so will slow down queries as iteration across mixed timestamps takes longer than if you only record one type or the other. OpenTSDB will store whatever you give it.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When writing to the telnet interface, timestamps may optionally be written in the form <code class="docutils literal notranslate"><span class="pre">1364410924.250</span></code>, where three digits representing the milliseconds are placed after a period.  Timestamps sent to the <code class="docutils literal notranslate"><span class="pre">/api/put</span></code> endpoint over HTTP <em>must</em> be integers and may not have periods. Data with millisecond resolution can only be extracted via the <code class="docutils literal notranslate"><span class="pre">/api/query</span></code> endpoint or CLI command at this time. See <span class="xref std std-doc">query/index</span> for details.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Providing millisecond resolution does not necessarily mean that OpenTSDB supports write speeds of 1 data point per millisecond over many time series. While a single TSD may be able to handle a few thousand writes per second, that would only cover a few time series if you’re trying to store a point every millisecond. Instead OpenTSDB aims to provide greater measurement accuracy and you should generally avoid recording data at such a speed, particularly for long running time series.</p>
</div>
</div>
<div class="section" id="metrics-and-tags">
<h3>Metrics and Tags</h3>
<p>The following rules apply to metric and tag values:</p>
<ul class="simple">
<li><p>Strings are case sensitive, i.e. “Sys.Cpu.User” will be stored separately from “sys.cpu.user”</p></li>
<li><p>Spaces are not allowed</p></li>
<li><p>Only the following characters are allowed: <code class="docutils literal notranslate"><span class="pre">a</span></code> to <code class="docutils literal notranslate"><span class="pre">z</span></code>, <code class="docutils literal notranslate"><span class="pre">A</span></code> to <code class="docutils literal notranslate"><span class="pre">Z</span></code>, <code class="docutils literal notranslate"><span class="pre">0</span></code> to <code class="docutils literal notranslate"><span class="pre">9</span></code>, <code class="docutils literal notranslate"><span class="pre">-</span></code>, <code class="docutils literal notranslate"><span class="pre">_</span></code>, <code class="docutils literal notranslate"><span class="pre">.</span></code>, <code class="docutils literal notranslate"><span class="pre">/</span></code> or Unicode letters (as per the specification)</p></li>
</ul>
<p>Metric and tags are not limited in length, though you should try to keep the values fairly short.</p>
</div>
<div class="section" id="integer-values">
<h3>Integer Values</h3>
<p>If the value from a <code class="docutils literal notranslate"><span class="pre">put</span></code> command is parsed without a decimal point (<code class="docutils literal notranslate"><span class="pre">.</span></code>), it will be treated as a signed integer. Integers are stored, unsigned, with variable length encoding so that a data point may take as little as 1 byte of space or up to 8 bytes. This means a data point can have a minimum value of -9,223,372,036,854,775,808 and a maximum value of 9,223,372,036,854,775,807 (inclusive). Integers cannot have commas or any character other than digits and the dash (for negative values).  For example, in order to store the maximum value, it must be provided in the form <code class="docutils literal notranslate"><span class="pre">9223372036854775807</span></code>.</p>
</div>
<div class="section" id="floating-point-values">
<h3>Floating Point Values</h3>
<p>If the value from a <code class="docutils literal notranslate"><span class="pre">put</span></code> command is parsed with a decimal point (<code class="docutils literal notranslate"><span class="pre">.</span></code>) it will be treated as a floating point value. Currently all floating point values are stored on 4 bytes, single-precision, with support for 8 byte double-precision in 2.4 and later.  Floats are stored in IEEE 754 floating-point “single format” with positive and negative value support.  Infinity and Not-a-Number values are not supported and will throw an error if supplied to a TSD. See <a class="reference external" href="https://en.wikipedia.org/wiki/IEEE_floating_point">Wikipedia</a> and the <a class="reference external" href="http://docs.oracle.com/javase/specs/jls/se7/html/jls-4.html#jls-4.2.3">Java Documentation</a> for details.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Because OpenTSDB only supports floating point values, it is not suitable for storing measurements that require exact values like currency. This is why, when storing a value like <code class="docutils literal notranslate"><span class="pre">15.2</span></code> the database may return <code class="docutils literal notranslate"><span class="pre">15.199999809265137</span></code>.</p>
</div>
</div>
<div class="section" id="ordering">
<h3>Ordering</h3>
<p>Unlike other solutions, OpenTSDB allows for writing data for a given time series in any order you want.  This enables significant flexibility in writing data to a TSD, allowing for populating current data from your systems, then importing historical data at a later time.</p>
</div>
<div class="section" id="duplicate-data-points">
<h3>Duplicate Data Points</h3>
<p>Writing data points in OpenTSDB is generally idempotent within an hour of the original write. This means  you can write the value <code class="docutils literal notranslate"><span class="pre">42</span></code> at timestamp <code class="docutils literal notranslate"><span class="pre">1356998400</span></code> and then write <code class="docutils literal notranslate"><span class="pre">42</span></code> again for the same time and nothing bad will happen. However if you have compactions enabled to reduce storage consumption and write the same data point after the row of data has been compacted, an exception may be returned when you query over that row. If you attempt to write two different values with the same timestamp, a duplicate data point exception may be thrown during query time. This is due to a difference in encoding integers on 1, 2, 4 or 8 bytes and floating point numbers. If the first value was an integer and the second a floating point, the duplicate error will always be thrown. However if both values were floats or they were both integers that could be encoded on the same length, then the original value may be overwritten if a compaction has not occurred on the row.</p>
<p>In most situations, if a duplicate data point is written it is usually an indication that something went wrong with the data source such as a process restarting unexpectedly or a bug in a script. OpenTSDB will fail “safe” by throwing an exception when you query over a row with one or more duplicates so you can down the issue.</p>
<p>With OpenTSDB 2.1 you can enable last-write-wins by setting the <code class="docutils literal notranslate"><span class="pre">tsd.storage.fix_duplicates</span></code> configuration value to <code class="docutils literal notranslate"><span class="pre">true</span></code>. With this flag enabled, at query time, the most recent value recorded will be returned instead of throwing an exception. A warning will also be written to the log file noting a duplicate was found. If compaction is also enabled, then the original compacted value will be overwritten with the latest value.</p>
</div>
</div>
<div class="section" id="input-methods">
<h2>Input Methods</h2>
<p>There are currently three main methods to get data into OpenTSDB: Telnet API, HTTP API and batch import from a file. Alternatively you can use a tool that provides OpenTSDB support, or if you’re extremely adventurous, use the Java library.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Don’t try to write directly to the underlying storage system, e.g. HBase. Just don’t. It’ll get messy quickly.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If the <code class="docutils literal notranslate"><span class="pre">tsd.mode</span></code> is set to <code class="docutils literal notranslate"><span class="pre">ro</span></code> instead of <code class="docutils literal notranslate"><span class="pre">rw</span></code>, the TSD will not accept data points through RPC calls. Telnet style calls will throw an exception and calls to the HTTP endpoint will return a 404 error. However it is still possible to write via the JAVA API when the mode is set to read only.</p>
</div>
<div class="section" id="telnet">
<h3>Telnet</h3>
<p>The easiest way to get started with OpenTSDB is to open up a terminal or telnet client, connect to your TSD and issue a <code class="docutils literal notranslate"><span class="pre">put</span></code> command and hit ‘enter’. If you are writing a program, simply open a socket, print the string command with a new line and send the packet. The telnet command format is:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="o">&lt;</span><span class="n">metric</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">timestamp</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">value</span><span class="o">&gt;</span> <span class="o">&lt;</span><span class="n">tagk1</span><span class="o">=</span><span class="n">tagv1</span><span class="p">[</span> <span class="n">tagk2</span><span class="o">=</span><span class="n">tagv2</span> <span class="o">...</span><span class="n">tagkN</span><span class="o">=</span><span class="n">tagvN</span><span class="p">]</span><span class="o">&gt;</span>
</pre></div>
</div>
<p>For example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">put</span> <span class="n">sys</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">user</span> <span class="mi">1356998400</span> <span class="mf">42.5</span> <span class="n">host</span><span class="o">=</span><span class="n">webserver01</span> <span class="n">cpu</span><span class="o">=</span><span class="mi">0</span>
</pre></div>
</div>
<p>Each <code class="docutils literal notranslate"><span class="pre">put</span></code> can only send a single data point. Don’t forget the newline character, e.g. <code class="docutils literal notranslate"><span class="pre">\n</span></code> at the end of your command.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The Telnet method of writing is discouraged as it doesn’t provide a way of determining which data points failed to write due to formatting or storage errors. Instead use the HTTP API.</p>
</div>
</div>
<div class="section" id="http-api">
<h3>Http API</h3>
<p>As of version 2.0, data can be sent over HTTP in formats supported by ‘Serializer’ plugins. Multiple, un-related data points can be sent in a single HTTP POST request to save bandwidth. See the <span class="xref std std-doc">../api_http/put</span> for details.</p>
</div>
<div class="section" id="batch-import">
<h3>Batch Import</h3>
<p>If you are importing data from another system or you need to backfill historical data, you can use the <code class="docutils literal notranslate"><span class="pre">import</span></code> CLI utility. See <span class="xref std std-doc">cli/import</span> for details.</p>
</div>
</div>
<div class="section" id="write-performance">
<h2>Write Performance</h2>
<p>OpenTSDB can scale to writing millions of data points per ‘second’ on commodity servers with regular spinning hard drives. However users who fire up a VM with HBase in stand-alone mode and try to slam millions of data points at a brand new TSD are disappointed when they can only write data in the hundreds of points per second. Here’s what you need to do to scale for brand new installs or testing and for expanding existing systems.</p>
<div class="section" id="uid-assignment">
<h3>UID Assignment</h3>
<p>The first sticking point folks run into is ‘’uid assignment’‘. Every string for a metric, tag key and tag value must be assigned a UID before the data point can be stored. For example, the metric <code class="docutils literal notranslate"><span class="pre">sys.cpu.user</span></code> may be assigned a UID of <code class="docutils literal notranslate"><span class="pre">000001</span></code> the first time it is encountered by a TSD. This assignment takes a fair amount of time as it must fetch an available UID, write a UID to name mapping and a name to UID mapping, then use the UID to write the data point’s row key. The UID will be stored in the TSD’s cache so that the next time the same metric comes through, it can find the UID very quickly.</p>
<p>Therefore, we recommend that you ‘pre-assign’ UID to as many metrics, tag keys and tag values as you can. If you have designed a naming schema as recommended above, you’ll know most of the values to assign. You can use the CLI tools <span class="xref std std-doc">cli/mkmetric</span>, <span class="xref std std-doc">cli/uid</span> or the HTTP API <span class="xref std std-doc">../api_http/uid/index</span> to perform pre-assignments. Any time you are about to send a bunch of new metrics or tags to a running OpenTSDB cluster, try to pre-assign or the TSDs will bog down a bit when they get the new data.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you restart a TSD, it will have to lookup the UID for every metric and tag so performance will be a little slow until the cache is filled.</p>
</div>
</div>
<div class="section" id="random-metric-uid-assignment">
<h3>Random Metric UID Assignment</h3>
<p>With 2.2 you can randomly assign UIDs to metrics for better region server write distribution. Because metric UIDs are located at the start of the row key, if a new set of busy metric are created, all writes for those metric will be on the same server until the region splits. With random UID generation enabled, the new metrics will be distributed across the key space and likely to wind up in different regions on different servers.</p>
<p>Random metric generation can be enabled or disabled at any time by modifying the <code class="docutils literal notranslate"><span class="pre">tsd.core.uid.random_metrics</span></code> flag and data is backwards compatible all the way back to OpenTSDB 1.0. However it is recommended that you pre-split your TSDB data table according to the full metric UID space. E.g. if you use the default UID size in OpenTSDB, UIDs are 3 bytes wide, thus you can have 16,777,215 values. If you already have data in your TSDB table and choose to enable random UIDs, you may want to create new regions.</p>
<p>When generating random IDs, TSDB will try up to 10 times to assign a UID without a collision. Thus as the number of assigned metrics increases so too will the number of collisions and the likely hood that a data point may be dropped due to retries. If you enable random IDs and keep adding more metrics then you may want to increase the number of bytes on metric UIDs. Note that the UID change is not backwards compatible so you have to create a new table and migrate your old data.</p>
</div>
<div class="section" id="salting">
<h3>Salting</h3>
<p>In 2.2 salting is supported to greatly increase write distribution across region servers. When enabled, a configured number of bytes are prepended to each row key. Each metric and combination of tags is then hashed into one “bucket”, the ID of which is written to the salt bytes. Distribution is improved particularly for high-cardinality metrics (those with a large number of tag combinations) as the time series are split across the configured bucket count, thus routed to different regions and different servers. For example, without salting, a metric with 1 million series will be written to a single region on a single server. With salting enabled and a bucket size of 20, the series will be split across 20 regions (and 20 servers if the cluster has that many hosts) where each region has 50,000 series.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Because salting modifies the storage format, you cannot enable or disable salting at whim. If you have existing data, you must start a new data table and migrate data from the old table into the new one. Salted data cannot be read from previous versions of OpenTSDB.</p>
</div>
<p>To enable salting you must modify the config file parameter <code class="docutils literal notranslate"><span class="pre">tsd.storage.salt.width</span></code> and optionally <code class="docutils literal notranslate"><span class="pre">tsd.storage.salt.buckets</span></code>. We recommend setting the salt width to <code class="docutils literal notranslate"><span class="pre">1</span></code> and determine the number of buckets based on a factor of the number of region servers in your cluster. Note that at query time, the TSD will fire <code class="docutils literal notranslate"><span class="pre">tsd.storage.salt.buckets</span></code> number of scanners to fetch data. The proper number of salt buckets must be determined through experimentation as at some point query performance may suffer due to having too many scanners open and collating the results. In the future the salt width and buckets may be configurable but we didn’t want folks changing settings on accident and losing data.</p>
</div>
<div class="section" id="appends">
<h3>Appends</h3>
<p>Also in 2.2, writing to HBase columns via appends is now supported. This can improve both read and write performance in that TSDs will no longer maintain a queue of rows to compact at the end of each hour, thus preventing a massive read and re-write operation in HBase. However due to the way appends operate in HBase, an increase in CPU utilization, store file size and HDFS traffic will occur on the region servers. Make sure to monitor your HBase servers closely.</p>
<p>At read time, only one column is returned per row similar to post-TSD-compaction rows. However note that if the <code class="docutils literal notranslate"><span class="pre">tsd.storage.repair_appends</span></code> is enabled, then when a column has duplicates or out of order data, it will be re-written to HBase. Also columns with many duplicates or ordering issues may slow queries as they must be resolved before answering the caller.</p>
<p>Appends can be enabled and disabled at any time. However versions of OpenTSDB prior to 2.2 will skip over appended values.</p>
</div>
<div class="section" id="pre-split-hbase-regions">
<h3>Pre-Split HBase Regions</h3>
<p>For brand new installs you will see much better performance if you pre-split the regions in HBase regardless of if you’re testing on a stand-alone server or running a full cluster. HBase regions handle a defined range of row keys and are essentially a single file. When you create the <code class="docutils literal notranslate"><span class="pre">tsdb</span></code> table and start writing data for the first time, all of those data points are being sent to this one file on one server. As a region fills up, HBase will automatically split it into different files and move it to other servers in the cluster, but when this happens, the TSDs cannot write to the region and must buffer the data points. Therefore, if you can pre-allocate a number of regions before you start writing, the TSDs can send data to multiple files or servers and you’ll be taking advantage of the linear scalability immediately.</p>
<p>The simplest way to pre-split your <code class="docutils literal notranslate"><span class="pre">tsdb</span></code> table regions is to estimate the number of unique metric names you’ll be recording. If you have designed a naming schema, you should have a pretty good idea. Let’s say that we will track 4,000 metrics in our system. That’s not to say 4,000 time series, as we’re not counting the tags yet, just the metric names such as “sys.cpu.user”. Data points are written in row keys where the metric’s UID comprises the first bytes, 3 bytes by default. The first metric will be assigned a UID of <code class="docutils literal notranslate"><span class="pre">000001</span></code> as a hex encoded value. The 4,000th metric will have a UID of <code class="docutils literal notranslate"><span class="pre">000FA0</span></code> in hex. You can use these as the start and end keys in the script from the <a class="reference external" href="http://hbase.apache.org/book/perf.writing.html">HBase Book</a> to split your table into any number of regions. 256 regions may be a good place to start depending on how many time series share each metric.</p>
<p><code class="docutils literal notranslate"><span class="pre">hbase</span> <span class="pre">org.apache.hadoop.hbase.util.RegionSplitter</span> <span class="pre">tsdb</span> <span class="pre">UniformSplit</span> <span class="pre">-c</span> <span class="pre">256</span> <span class="pre">-f</span> <span class="pre">t</span></code></p>
<p>The simple split method above assumes that you have roughly an equal number of time series per metric (i.e. a fairly consistent cardinality). E.g. the metric with a UID of <code class="docutils literal notranslate"><span class="pre">000001</span></code> may have 200 time series and <code class="docutils literal notranslate"><span class="pre">000FA0</span></code> has about 150. If you have a wide range of time series per metric, e.g. <code class="docutils literal notranslate"><span class="pre">000001</span></code> has 10,000 time series while <code class="docutils literal notranslate"><span class="pre">000FA0</span></code> only has 2, you may need to develop a more complex splitting algorithm.</p>
<p>But don’t worry too much about splitting. As stated above, HBase will automatically split regions for you so over time, the data will be distributed fairly evenly.</p>
</div>
<div class="section" id="distributed-hbase">
<h3>Distributed HBase</h3>
<p>HBase will run in stand-alone mode where it will use the local file system for storing files. It will still use multiple regions and perform as well as the underlying disk or raid array will let it. You’ll definitely want a RAID array under HBase so that if a drive fails, you can replace it without losing data. This kind of setup is fine for testing or very small installations and you should be able to get into the low thousands of data points per second.</p>
<p>However if you want serious throughput and scalability you have to setup a Hadoop and HBase cluster with multiple servers. In a distributed setup HDFS manages region files, automatically distributing copies to different servers for fault tolerance. HBase assigns regions to different servers and OpenTSDB’s client will send data points to the specific server where they will be stored. You’re now spreading operations amongst multiple servers, increasing performance and storage. If you need even more throughput or storage, just add nodes or disks.</p>
<p>There are a number of ways to setup a Hadoop/HBase cluster and a ton of various tuning tweaks to make, so Google around and ask user groups for advice. Some general recommendations include:</p>
<ul class="simple">
<li><p>Dedicate a pair of high memory, low disk space servers for the Name Node. Set them up for high availability using something like Heartbeat and Pacemaker.</p></li>
<li><p>Setup Zookeeper on at least 3 servers for fault tolerance. They must have a lot of RAM and a fairly fast disk for log writing. On small clusters, these can run on the Name node servers.</p></li>
<li><p>JBOD for the HDFS data nodes</p></li>
<li><p>HBase region servers can be collocated with the HDFS data nodes</p></li>
<li><p>At least 1 gbps links between servers, 10 gbps preferable.</p></li>
<li><p>Keep the cluster in a single data center</p></li>
</ul>
</div>
<div class="section" id="multiple-tsds">
<h3>Multiple TSDs</h3>
<p>A single TSD can handle thousands of writes per second. But if you have many sources it’s best to scale by running multiple TSDs and using a load balancer (such as Varnish or DNS round robin) to distribute the writes. Many users colocate TSDs on their HBase region servers when the cluster is dedicated to OpenTSDB.</p>
</div>
<div class="section" id="persistent-connections">
<h3>Persistent Connections</h3>
<p>Enable keep-alives in the TSDs and make sure that any applications you are using to send time series data keep their connections open instead of opening and closing for every write. See <span class="xref std std-doc">configuration</span> for details.</p>
</div>
<div class="section" id="disable-meta-data-and-real-time-publishing">
<h3>Disable Meta Data and Real Time Publishing</h3>
<p>OpenTSDB 2.0 introduced meta data for tracking the kinds of data in the system. When tracking is enabled, a counter is incremented for every data point written and new UIDs or time series will generate meta data. The data may be pushed to a search engine or passed through tree generation code. These processes require greater memory in the TSD and may affect throughput. Tracking is disabled by default so test it out before enabling the feature.</p>
<p>2.0 also introduced a real-time publishing plugin where incoming data points can be emitted to another destination immediately after they’re queued for storage. This is disabled by default so test any plugins you are interested in before deploying in production.</p>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../query/index.html" title="Querying or Reading Data"
             >next</a> |</li>
        <li class="right" >
          <a href="../configuration.html" title="Configuration"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="../../index.html">OpenTSDB 2.4 documentation</a> &#187;</li>
          <li class="nav-item nav-item-1"><a href="../index.html" >User Guide</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2019, OpenTSDB.
    Created using <a href="http://sphinx.pocoo.org/">Sphinx</a>. Theme by <a href="http://github.com/vkvn">vkvn</a>
    </div>
  </body>
</html>